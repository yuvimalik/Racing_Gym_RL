{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Racecar Gym PPO Training - Google Colab\n",
    "\n",
    "This notebook provides a complete setup and training pipeline for training a PPO agent on the Racecar Gym environment.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Make sure GPU is enabled: Runtime -> Change runtime type -> GPU\n",
    "2. Run all cells in order\n",
    "3. Training progress will be displayed in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q gymnasium>=0.29.0\n",
    "!pip install -q stable-baselines3[extra]>=2.0.0\n",
    "!pip install -q numpy>=1.24.0\n",
    "!pip install -q pybullet>=3.2.0\n",
    "!pip install -q matplotlib>=3.7.0\n",
    "!pip install -q opencv-python>=4.8.0\n",
    "!pip install -q tensorboard>=2.13.0\n",
    "!pip install -q pyyaml>=6.0\n",
    "!pip install -q git+https://github.com/axelbr/racecar_gym.git\n",
    "\n",
    "print(\"All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Repository and Setup\n",
    "\n",
    "If you're using this notebook with a GitHub repository, clone it here. Otherwise, upload the necessary files manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Clone from GitHub (uncomment and modify if needed)\n",
    "# !git clone https://github.com/yourusername/Racing_Gym_RL.git\n",
    "# %cd Racing_Gym_RL\n",
    "\n",
    "# Option 2: Upload files manually using the file browser\n",
    "# Create necessary directories\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.makedirs('config', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "print(\"Directories created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration file if it doesn't exist\n",
    "config_content = \"\"\"# Racecar Gym PPO Training Configuration\n",
    "\n",
    "# Environment settings\n",
    "environment:\n",
    "  track: circle\n",
    "  scenario: null\n",
    "  render_mode: null\n",
    "  render_options:\n",
    "    width: 320\n",
    "    height: 240\n",
    "\n",
    "# Agent configuration\n",
    "agent:\n",
    "  sensors: [lidar, pose, velocity, acceleration]\n",
    "  actuators: [motor, steering]\n",
    "  color: blue\n",
    "\n",
    "# PPO hyperparameters\n",
    "ppo:\n",
    "  learning_rate: 3.0e-4\n",
    "  n_steps: 2048\n",
    "  batch_size: 64\n",
    "  n_epochs: 10\n",
    "  gamma: 0.99\n",
    "  gae_lambda: 0.95\n",
    "  clip_range: 0.2\n",
    "  ent_coef: 0.01\n",
    "  vf_coef: 0.5\n",
    "  max_grad_norm: 0.5\n",
    "  use_sde: false\n",
    "  sde_sample_freq: -1\n",
    "\n",
    "# Policy network architecture\n",
    "policy:\n",
    "  net_arch: [256, 256]\n",
    "  activation_fn: tanh\n",
    "\n",
    "# Training settings\n",
    "training:\n",
    "  total_timesteps: 500000  # Reduced for Colab demo\n",
    "  eval_freq: 10000\n",
    "  n_eval_episodes: 5\n",
    "  save_freq: 50000\n",
    "  log_interval: 10\n",
    "\n",
    "# Paths\n",
    "paths:\n",
    "  model_dir: ./models\n",
    "  log_dir: ./logs\n",
    "  results_dir: ./results\n",
    "\n",
    "# Device\n",
    "device: auto\n",
    "\"\"\"\n",
    "\n",
    "if not os.path.exists('config/circle_config.yaml'):\n",
    "    with open('config/circle_config.yaml', 'w') as f:\n",
    "        f.write(config_content)\n",
    "    print(\"Configuration file created!\")\n",
    "else:\n",
    "    print(\"Configuration file already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Import necessary libraries and set up the training environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import gymnasium as gym\n",
    "import racecar_gym.envs.gym_api\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('config/circle_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "set_random_seed(seed)\n",
    "\n",
    "# Create directories\n",
    "model_dir = Path(config['paths']['model_dir'])\n",
    "log_dir = Path(config['paths']['log_dir'])\n",
    "results_dir = Path(config['paths']['results_dir'])\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Determine device\n",
    "device_config = config.get('device', 'auto')\n",
    "if device_config == 'auto':\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "else:\n",
    "    device = device_config\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Model directory: {model_dir}\")\n",
    "print(f\"Log directory: {log_dir}\")\n",
    "print(f\"Results directory: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment\n",
    "\n",
    "Create the Racecar Gym environment. On first run, tracks will be downloaded automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env_config = config['environment']\n",
    "track = env_config['track']\n",
    "env_id = f'SingleAgent{track.capitalize()}-v0'\n",
    "\n",
    "print(f\"Creating environment: {env_id}\")\n",
    "\n",
    "# Helper function to create training environment\n",
    "def make_train_env():\n",
    "    env = gym.make(\n",
    "        env_id,\n",
    "        render_mode=env_config.get('render_mode'),\n",
    "        render_options=env_config.get('render_options')\n",
    "    )\n",
    "    env = Monitor(env, filename=str(log_dir / 'monitor_0'))\n",
    "    env.reset(seed=seed)\n",
    "    return env\n",
    "\n",
    "# Helper function to create evaluation environment\n",
    "def make_eval_env():\n",
    "    env = gym.make(\n",
    "        env_id,\n",
    "        render_mode='rgb_array',\n",
    "        render_options=env_config.get('render_options')\n",
    "    )\n",
    "    env = Monitor(env, filename=str(log_dir / 'monitor_eval'))\n",
    "    env.reset(seed=seed + 1000)\n",
    "    return env\n",
    "\n",
    "# Wrap in vectorized environment\n",
    "env = DummyVecEnv([make_train_env])\n",
    "\n",
    "# Create evaluation environment\n",
    "eval_env = DummyVecEnv([make_eval_env])\n",
    "\n",
    "print(f\"Observation space: {env.envs[0].observation_space}\")\n",
    "print(f\"Action space: {env.envs[0].action_space}\")\n",
    "print(\"Environment created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PPO Model\n",
    "\n",
    "Initialize the PPO model with appropriate policy for Dict observation spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine policy type based on observation space\n",
    "if isinstance(env.envs[0].observation_space, gym.spaces.Dict):\n",
    "    policy = 'MultiInputPolicy'\n",
    "    print(\"Using MultiInputPolicy for Dict observation space\")\n",
    "else:\n",
    "    policy = 'MlpPolicy'\n",
    "    print(\"Using MlpPolicy for Box observation space\")\n",
    "\n",
    "# Get activation function\n",
    "activation_fn_map = {\n",
    "    'tanh': torch.nn.Tanh,\n",
    "    'relu': torch.nn.ReLU,\n",
    "    'elu': torch.nn.ELU\n",
    "}\n",
    "activation_fn = activation_fn_map.get(\n",
    "    config['policy'].get('activation_fn', 'tanh'),\n",
    "    torch.nn.Tanh\n",
    ")\n",
    "\n",
    "# Create PPO model\n",
    "ppo_config = config['ppo']\n",
    "policy_config = config['policy']\n",
    "\n",
    "model = PPO(\n",
    "    policy=policy,\n",
    "    env=env,\n",
    "    learning_rate=ppo_config['learning_rate'],\n",
    "    n_steps=ppo_config['n_steps'],\n",
    "    batch_size=ppo_config['batch_size'],\n",
    "    n_epochs=ppo_config['n_epochs'],\n",
    "    gamma=ppo_config['gamma'],\n",
    "    gae_lambda=ppo_config['gae_lambda'],\n",
    "    clip_range=ppo_config['clip_range'],\n",
    "    ent_coef=ppo_config['ent_coef'],\n",
    "    vf_coef=ppo_config['vf_coef'],\n",
    "    max_grad_norm=ppo_config['max_grad_norm'],\n",
    "    use_sde=ppo_config.get('use_sde', False),\n",
    "    sde_sample_freq=ppo_config.get('sde_sample_freq', -1),\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=policy_config['net_arch'],\n",
    "        activation_fn=activation_fn\n",
    "    ),\n",
    "    device=device,\n",
    "    verbose=1,\n",
    "    tensorboard_log=str(log_dir)\n",
    ")\n",
    "\n",
    "print(\"PPO model created successfully!\")\n",
    "print(f\"Policy: {model.policy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Start training the PPO agent. This may take a while depending on the number of timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "training_config = config['training']\n",
    "\n",
    "# Evaluation callback\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=str(model_dir / 'best_model'),\n",
    "    log_path=str(log_dir),\n",
    "    eval_freq=training_config['eval_freq'],\n",
    "    n_eval_episodes=training_config['n_eval_episodes'],\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Checkpoint callback\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=training_config['save_freq'],\n",
    "    save_path=str(model_dir),\n",
    "    name_prefix='ppo_racecar'\n",
    ")\n",
    "\n",
    "# Combine callbacks\n",
    "callbacks = CallbackList([eval_callback, checkpoint_callback])\n",
    "\n",
    "print(\"Callbacks configured\")\n",
    "print(f\"Total timesteps: {training_config['total_timesteps']}\")\n",
    "print(f\"Evaluation frequency: {training_config['eval_freq']}\")\n",
    "print(f\"Save frequency: {training_config['save_freq']}\")\n",
    "print(\"\\nStarting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.learn(\n",
    "    total_timesteps=training_config['total_timesteps'],\n",
    "    callback=callbacks,\n",
    "    log_interval=training_config.get('log_interval', 10),\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = model_dir / 'final_model'\n",
    "model.save(str(final_model_path))\n",
    "print(f\"\\nTraining complete! Final model saved to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate the trained model and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model (or use final_model)\n",
    "model_path = model_dir / 'best_model' / 'best_model.zip'\n",
    "if not model_path.exists():\n",
    "    model_path = model_dir / 'final_model.zip'\n",
    "\n",
    "print(f\"Loading model from {model_path}\")\n",
    "trained_model = PPO.load(str(model_path))\n",
    "\n",
    "# Evaluate the model\n",
    "n_eval_episodes = 5\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "eval_env_test = gym.make(\n",
    "    env_id,\n",
    "    render_mode='rgb_array',\n",
    "    render_options=env_config.get('render_options')\n",
    ")\n",
    "\n",
    "for episode in range(n_eval_episodes):\n",
    "    obs, info = eval_env_test.reset(seed=seed + episode)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = trained_model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = eval_env_test.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(episode_length)\n",
    "    print(f\"Episode {episode + 1}: Reward={episode_reward:.2f}, Length={episode_length}\")\n",
    "\n",
    "eval_env_test.close()\n",
    "\n",
    "print(f\"\\nMean Reward: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
    "print(f\"Mean Episode Length: {np.mean(episode_lengths):.1f} ± {np.std(episode_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress\n",
    "\n",
    "Load TensorBoard logs to visualize training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Start TensorBoard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results\n",
    "\n",
    "Download trained models and results to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file of results\n",
    "import shutil\n",
    "\n",
    "results_zip = 'racecar_training_results.zip'\n",
    "shutil.make_archive('racecar_training_results', 'zip', model_dir.parent)\n",
    "\n",
    "print(f\"Results zipped: {results_zip}\")\n",
    "print(\"\\nTo download:\")\n",
    "print(\"1. Use the file browser on the left\")\n",
    "print(\"2. Right-click on racecar_training_results.zip\")\n",
    "print(\"3. Select 'Download'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
