{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Car Racing PPO - Evaluation & Training\n",
        "\n",
        "This notebook allows you to:\n",
        "1. **Evaluate** a trained model (upload your `.zip` file) and see stats/video.\n",
        "2. **Train** a new PPO agent from scratch.\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. Make sure GPU is enabled: Runtime -> Change runtime type -> GPU\n",
        "2. Run the **Install Dependencies** cell first.\n",
        "3. Choose your path: **Evaluation** or **Training** below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies\n",
        "!pip install -q \"numpy>=1.22.0,<1.23.0\"\n",
        "!pip install -q gym==0.17.3\n",
        "!pip install -q stable-baselines3[extra]==1.8.0\n",
        "!pip install -q matplotlib>=3.7.0 opencv-python>=4.8.0 tensorboard>=2.13.0 pyyaml>=6.0 pyglet==1.5.27 moviepy\n",
        "!pip install -q git+https://github.com/igilitschenski/multi_car_racing.git\n",
        "\n",
        "# Fix for Colab display\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "\n",
        "print(\"All dependencies installed successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup Environment and Wrapper (Run this cell!)\n",
        "import gym\n",
        "import gym_multi_car_racing\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
        "from IPython.display import Video, display\n",
        "\n",
        "class SingleAgentWrapper(gym.Wrapper):\n",
        "    \"\"\"Wrap MultiCarRacing to expose a single-agent view.\"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_space = env.observation_space\n",
        "        act_space = env.action_space\n",
        "\n",
        "        if len(obs_space.shape) == 4 and obs_space.shape[0] == 1:\n",
        "            self.observation_space = gym.spaces.Box(\n",
        "                low=obs_space.low[0],\n",
        "                high=obs_space.high[0],\n",
        "                shape=obs_space.shape[1:],\n",
        "                dtype=obs_space.dtype\n",
        "            )\n",
        "        if len(act_space.shape) == 2 and act_space.shape[0] == 1:\n",
        "            self.action_space = gym.spaces.Box(\n",
        "                low=act_space.low[0],\n",
        "                high=act_space.high[0],\n",
        "                shape=act_space.shape[1:],\n",
        "                dtype=act_space.dtype\n",
        "            )\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        # Gym 0.17.3 reset() returns just obs\n",
        "        obs = self.env.reset(**kwargs)\n",
        "        if hasattr(obs, \"shape\") and len(obs.shape) == 4 and obs.shape[0] == 1:\n",
        "            obs = obs[0]\n",
        "        elif isinstance(obs, (list, tuple)) and len(obs) == 1:\n",
        "            obs = obs[0]\n",
        "        return obs\n",
        "\n",
        "    def step(self, action):\n",
        "        if hasattr(self.env.action_space, \"shape\") and len(self.env.action_space.shape) == 2:\n",
        "            action = action.reshape(1, -1)\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        if hasattr(obs, \"shape\") and len(obs.shape) == 4 and obs.shape[0] == 1:\n",
        "            obs = obs[0]\n",
        "        elif isinstance(obs, (list, tuple)) and len(obs) == 1:\n",
        "            obs = obs[0]\n",
        "        if isinstance(reward, (list, tuple)) or (hasattr(reward, \"shape\") and len(reward.shape) > 0 and reward.shape[0] == 1):\n",
        "            reward = float(reward[0] if isinstance(reward, (list, tuple)) else reward[0])\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def render(self, mode='human', **kwargs):\n",
        "        out = self.env.render(mode=mode, **kwargs)\n",
        "        if hasattr(out, \"shape\") and len(out.shape) == 4 and out.shape[0] == 1:\n",
        "            out = out[0]\n",
        "        return out\n",
        "\n",
        "def create_eval_env():\n",
        "    env = gym.make(\n",
        "        'MultiCarRacing-v0',\n",
        "        num_agents=1,\n",
        "        direction='CCW',\n",
        "        use_random_direction=True,\n",
        "        backwards_flag=True,\n",
        "        h_ratio=0.25,\n",
        "        use_ego_color=False\n",
        "    )\n",
        "    env = SingleAgentWrapper(env)\n",
        "    return env\n",
        "\n",
        "print(\"Environment classes defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 1: Evaluate Uploaded Model\n",
        "\n",
        "Use this section to upload your trained model (`.zip`) and see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"Please upload your model file (.zip file, e.g., final_model.zip)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    model_filename = list(uploaded.keys())[0]\n",
        "    print(f\"\\nModel uploaded: {model_filename}\")\n",
        "else:\n",
        "    print(\"No file uploaded.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run Evaluation\n",
        "if 'model_filename' in locals():\n",
        "    print(f\"Loading model: {model_filename}...\")\n",
        "    \n",
        "    # Load model\n",
        "    model = PPO.load(model_filename)\n",
        "    \n",
        "    # Create evaluation environment\n",
        "    eval_env = DummyVecEnv([create_eval_env])\n",
        "    eval_env = VecTransposeImage(eval_env)\n",
        "    \n",
        "    # Config\n",
        "    n_episodes = 3\n",
        "    video_path = 'eval_video.mp4'\n",
        "    \n",
        "    print(f\"Evaluating for {n_episodes} episodes...\")\n",
        "    \n",
        "    # Video Writer\n",
        "    obs = eval_env.reset()\n",
        "    # Get formatting from first frame\n",
        "    temp_env = create_eval_env()\n",
        "    temp_env.reset()\n",
        "    frame = temp_env.render(mode='rgb_array')\n",
        "    height, width, _ = frame.shape\n",
        "    temp_env.close()\n",
        "    \n",
        "    out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (width, height))\n",
        "    \n",
        "    episode_rewards = []\n",
        "    \n",
        "    for val_ep in range(n_episodes):\n",
        "        obs = eval_env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        \n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, info = eval_env.step(action)\n",
        "            total_reward += float(reward[0])\n",
        "            \n",
        "            # Render frame\n",
        "            # Access the underlying env to get the render\n",
        "            frame = eval_env.envs[0].render(mode='rgb_array')\n",
        "            if frame is not None:\n",
        "                # Convert RGB to BGR for OpenCV\n",
        "                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                out.write(frame_bgr)\n",
        "        \n",
        "        episode_rewards.append(total_reward)\n",
        "        print(f\"Episode {val_ep+1}: Reward = {total_reward:.2f}\")\n",
        "    \n",
        "    out.release()\n",
        "    eval_env.close()\n",
        "    \n",
        "    print(f\"\\nMean Reward: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}\")\n",
        "    print(f\"Video saved to {video_path}\")\n",
        "    \n",
        "    # Display Video\n",
        "    print(\"\\n Displaying Video:\")\n",
        "    display(Video(video_path, embed=True, html_attributes='controls loop autoplay'))\n",
        "else:\n",
        "    print(\"Please upload a model first!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 2: Train New Model\n",
        "\n",
        "Run the cells below to train a new model from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from stable_baselines3.common.callbacks import CheckpointCallback\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "import torch\n",
        "\n",
        "# Initialize Config\n",
        "seed = 42\n",
        "set_random_seed(seed)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Directories\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('logs', exist_ok=True)\n",
        "\n",
        "# Create Env\n",
        "env = DummyVecEnv([create_eval_env])  # Re-use the creator\n",
        "env = VecTransposeImage(env)\n",
        "\n",
        "# Create Model\n",
        "model = PPO(\n",
        "    policy='CnnPolicy',\n",
        "    env=env,\n",
        "    learning_rate=3e-4,\n",
        "    n_steps=2048,\n",
        "    batch_size=64,\n",
        "    n_epochs=10,\n",
        "    gamma=0.99,\n",
        "    gae_lambda=0.95,\n",
        "    clip_range=0.2,\n",
        "    ent_coef=0.01,\n",
        "    vf_coef=0.5,\n",
        "    max_grad_norm=0.5,\n",
        "    device=device,\n",
        "    verbose=1,\n",
        "    tensorboard_log='./logs'\n",
        ")\n",
        "\n",
        "print(f\"Training on {device}...\")\n",
        "\n",
        "# Callback\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=50000,\n",
        "    save_path='./models',\n",
        "    name_prefix='ppo_racecar'\n",
        ")\n",
        "\n",
        "# Train\n",
        "model.learn(\n",
        "    total_timesteps=100000,  # Adjust as needed\n",
        "    callback=checkpoint_callback,\n",
        "    progress_bar=True\n",
        ")\n",
        "\n",
        "model.save(\"final_model\")\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize with TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}