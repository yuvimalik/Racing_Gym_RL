{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Car Racing PPO Training - Google Colab\n",
        "\n",
        "This notebook provides a complete setup and training pipeline for training a PPO agent on the Multi-Car Racing environment.\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. Make sure GPU is enabled: Runtime -> Change runtime type -> GPU\n",
        "2. Run all cells in order\n",
        "3. Training progress will be displayed in real-time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies\n",
        "!pip install -q \"numpy>=1.22.0,<1.23.0\"\n",
        "!pip install -q gym==0.17.3\n",
        "!pip install -q stable-baselines3[extra]==1.8.0\n",
        "!pip install -q matplotlib>=3.7.0 opencv-python>=4.8.0 tensorboard>=2.13.0 pyyaml>=6.0 pyglet==1.5.27\n",
        "!pip install -q git+https://github.com/igilitschenski/multi_car_racing.git\n",
        "\n",
        "print(\"All dependencies installed successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    print(\"GPU not available, using CPU\")\n",
        "    device = \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clone Repository and Setup\n",
        "\n",
        "If you're using this notebook with a GitHub repository, clone it here. Otherwise, upload the necessary files manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Option 1: Clone from GitHub (uncomment and modify if needed)\n",
        "# !git clone https://github.com/yourusername/Racing_Gym_RL.git\n",
        "# %cd Racing_Gym_RL\n",
        "\n",
        "# Option 2: Upload files manually using the file browser\n",
        "# Create necessary directories\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "os.makedirs('config', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('logs', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "print(\"Directories created!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create configuration file if it doesn't exist\n",
        "config_content = \"\"\"# Multi-Car Racing PPO Training Configuration\n",
        "\n",
        "environment:\n",
        "  env_id: MultiCarRacing-v0\n",
        "  num_agents: 1\n",
        "  direction: CCW\n",
        "  use_random_direction: true\n",
        "  backwards_flag: true\n",
        "  h_ratio: 0.25\n",
        "  use_ego_color: false\n",
        "  render_mode: null\n",
        "\n",
        "ppo:\n",
        "  learning_rate: 3.0e-4\n",
        "  n_steps: 2048\n",
        "  batch_size: 64\n",
        "  n_epochs: 10\n",
        "  gamma: 0.99\n",
        "  gae_lambda: 0.95\n",
        "  clip_range: 0.2\n",
        "  ent_coef: 0.01\n",
        "  vf_coef: 0.5\n",
        "  max_grad_norm: 0.5\n",
        "\n",
        "policy:\n",
        "  policy_type: CnnPolicy\n",
        "\n",
        "training:\n",
        "  total_timesteps: 500000  # Reduced for Colab demo\n",
        "  eval_freq: 10000\n",
        "  n_eval_episodes: 5\n",
        "  save_freq: 50000\n",
        "  log_interval: 10\n",
        "\n",
        "paths:\n",
        "  model_dir: ./models\n",
        "  log_dir: ./logs\n",
        "  results_dir: ./results\n",
        "\n",
        "device: auto\n",
        "\"\"\"\n",
        "\n",
        "if not os.path.exists('config/multi_car_config.yaml'):\n",
        "    with open('config/multi_car_config.yaml', 'w') as f:\n",
        "        f.write(config_content)\n",
        "    print(\"Configuration file created!\")\n",
        "else:\n",
        "    print(\"Configuration file already exists\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup\n",
        "\n",
        "Import necessary libraries and set up the training environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import gym\n",
        "import gym_multi_car_racing\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load configuration\n",
        "with open('config/multi_car_config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "set_random_seed(seed)\n",
        "\n",
        "# Create directories\n",
        "model_dir = Path(config['paths']['model_dir'])\n",
        "log_dir = Path(config['paths']['log_dir'])\n",
        "results_dir = Path(config['paths']['results_dir'])\n",
        "model_dir.mkdir(parents=True, exist_ok=True)\n",
        "log_dir.mkdir(parents=True, exist_ok=True)\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Determine device\n",
        "device_config = config.get('device', 'auto')\n",
        "if device_config == 'auto':\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "else:\n",
        "    device = device_config\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Model directory: {model_dir}\")\n",
        "print(f\"Log directory: {log_dir}\")\n",
        "print(f\"Results directory: {results_dir}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Environment\n",
        "\n",
        "Create the Racecar Gym environment. On first run, tracks will be downloaded automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create environment\n",
        "env_config = config['environment']\n",
        "env_id = env_config.get('env_id', 'MultiCarRacing-v0')\n",
        "\n",
        "print(f\"Creating environment: {env_id}\")\n",
        "\n",
        "class SingleAgentWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_space = env.observation_space\n",
        "        act_space = env.action_space\n",
        "\n",
        "        if len(obs_space.shape) == 4 and obs_space.shape[0] == 1:\n",
        "            self.observation_space = gym.spaces.Box(\n",
        "                low=obs_space.low[0],\n",
        "                high=obs_space.high[0],\n",
        "                shape=obs_space.shape[1:],\n",
        "                dtype=obs_space.dtype\n",
        "            )\n",
        "        if len(act_space.shape) == 2 and act_space.shape[0] == 1:\n",
        "            self.action_space = gym.spaces.Box(\n",
        "                low=act_space.low[0],\n",
        "                high=act_space.high[0],\n",
        "                shape=act_space.shape[1:],\n",
        "                dtype=act_space.dtype\n",
        "            )\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        if hasattr(obs, \"shape\") and obs.shape[0] == 1:\n",
        "            obs = obs[0]\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        if hasattr(self.env.action_space, \"shape\") and len(self.env.action_space.shape) == 2:\n",
        "            action = action.reshape(1, -1)\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        if hasattr(obs, \"shape\") and obs.shape[0] == 1:\n",
        "            obs = obs[0]\n",
        "        if hasattr(reward, \"shape\"):\n",
        "            reward = float(reward[0])\n",
        "        return obs, reward, done, info\n",
        "\n",
        "# Helper function to create training environment\n",
        "def make_train_env():\n",
        "    env = gym.make(\n",
        "        env_id,\n",
        "        num_agents=env_config.get('num_agents', 1),\n",
        "        direction=env_config.get('direction', 'CCW'),\n",
        "        use_random_direction=env_config.get('use_random_direction', True),\n",
        "        backwards_flag=env_config.get('backwards_flag', True),\n",
        "        h_ratio=env_config.get('h_ratio', 0.25),\n",
        "        use_ego_color=env_config.get('use_ego_color', False),\n",
        "        render_mode=env_config.get('render_mode')\n",
        "    )\n",
        "    if env_config.get('num_agents', 1) == 1:\n",
        "        env = SingleAgentWrapper(env)\n",
        "    env = Monitor(env, filename=str(log_dir / 'monitor_0'))\n",
        "    env.reset(seed=seed)\n",
        "    return env\n",
        "\n",
        "# Helper function to create evaluation environment\n",
        "def make_eval_env():\n",
        "    env = gym.make(\n",
        "        env_id,\n",
        "        num_agents=env_config.get('num_agents', 1),\n",
        "        direction=env_config.get('direction', 'CCW'),\n",
        "        use_random_direction=env_config.get('use_random_direction', True),\n",
        "        backwards_flag=env_config.get('backwards_flag', True),\n",
        "        h_ratio=env_config.get('h_ratio', 0.25),\n",
        "        use_ego_color=env_config.get('use_ego_color', False),\n",
        "        render_mode='rgb_array'\n",
        "    )\n",
        "    if env_config.get('num_agents', 1) == 1:\n",
        "        env = SingleAgentWrapper(env)\n",
        "    env = Monitor(env, filename=str(log_dir / 'monitor_eval'))\n",
        "    env.reset(seed=seed + 1000)\n",
        "    return env\n",
        "\n",
        "# Wrap in vectorized environment\n",
        "env = DummyVecEnv([make_train_env])\n",
        "env = VecTransposeImage(env)\n",
        "\n",
        "# Create evaluation environment\n",
        "eval_env = DummyVecEnv([make_eval_env])\n",
        "eval_env = VecTransposeImage(eval_env)\n",
        "\n",
        "print(f\"Observation space: {env.envs[0].observation_space}\")\n",
        "print(f\"Action space: {env.envs[0].action_space}\")\n",
        "print(\"Environment created successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create PPO Model\n",
        "\n",
        "Initialize the PPO model with appropriate policy for Dict observation spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use CNN policy for image observations\n",
        "policy = config['policy'].get('policy_type', 'CnnPolicy')\n",
        "print(f\"Using policy: {policy}\")\n",
        "\n",
        "# Create PPO model\n",
        "ppo_config = config['ppo']\n",
        "\n",
        "model = PPO(\n",
        "    policy=policy,\n",
        "    env=env,\n",
        "    learning_rate=ppo_config['learning_rate'],\n",
        "    n_steps=ppo_config['n_steps'],\n",
        "    batch_size=ppo_config['batch_size'],\n",
        "    n_epochs=ppo_config['n_epochs'],\n",
        "    gamma=ppo_config['gamma'],\n",
        "    gae_lambda=ppo_config['gae_lambda'],\n",
        "    clip_range=ppo_config['clip_range'],\n",
        "    ent_coef=ppo_config['ent_coef'],\n",
        "    vf_coef=ppo_config['vf_coef'],\n",
        "    max_grad_norm=ppo_config['max_grad_norm'],\n",
        "    device=device,\n",
        "    verbose=1,\n",
        "    tensorboard_log=str(log_dir)\n",
        ")\n",
        "\n",
        "print(\"PPO model created successfully!\")\n",
        "print(f\"Policy: {model.policy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "Start training the PPO agent. This may take a while depending on the number of timesteps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup callbacks\n",
        "training_config = config['training']\n",
        "\n",
        "# Evaluation callback\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path=str(model_dir / 'best_model'),\n",
        "    log_path=str(log_dir),\n",
        "    eval_freq=training_config['eval_freq'],\n",
        "    n_eval_episodes=training_config['n_eval_episodes'],\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "\n",
        "# Checkpoint callback\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=training_config['save_freq'],\n",
        "    save_path=str(model_dir),\n",
        "    name_prefix='ppo_racecar'\n",
        ")\n",
        "\n",
        "# Combine callbacks\n",
        "callbacks = CallbackList([eval_callback, checkpoint_callback])\n",
        "\n",
        "print(\"Callbacks configured\")\n",
        "print(f\"Total timesteps: {training_config['total_timesteps']}\")\n",
        "print(f\"Evaluation frequency: {training_config['eval_freq']}\")\n",
        "print(f\"Save frequency: {training_config['save_freq']}\")\n",
        "print(\"\\nStarting training...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train the model\n",
        "model.learn(\n",
        "    total_timesteps=training_config['total_timesteps'],\n",
        "    callback=callbacks,\n",
        "    log_interval=training_config.get('log_interval', 10),\n",
        "    progress_bar=True\n",
        ")\n",
        "\n",
        "# Save final model\n",
        "final_model_path = model_dir / 'final_model'\n",
        "model.save(str(final_model_path))\n",
        "print(f\"\\nTraining complete! Final model saved to {final_model_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "Evaluate the trained model and visualize results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the best model (or use final_model)\n",
        "model_path = model_dir / 'best_model' / 'best_model.zip'\n",
        "if not model_path.exists():\n",
        "    model_path = model_dir / 'final_model.zip'\n",
        "\n",
        "print(f\"Loading model from {model_path}\")\n",
        "trained_model = PPO.load(str(model_path))\n",
        "\n",
        "# Evaluate the model\n",
        "n_eval_episodes = 5\n",
        "episode_rewards = []\n",
        "episode_lengths = []\n",
        "\n",
        "# Build eval env (single-agent wrapper + transpose)\n",
        "raw_env = gym.make(\n",
        "    env_id,\n",
        "    num_agents=env_config.get('num_agents', 1),\n",
        "    direction=env_config.get('direction', 'CCW'),\n",
        "    use_random_direction=env_config.get('use_random_direction', True),\n",
        "    backwards_flag=env_config.get('backwards_flag', True),\n",
        "    h_ratio=env_config.get('h_ratio', 0.25),\n",
        "    use_ego_color=env_config.get('use_ego_color', False),\n",
        "    render_mode='rgb_array'\n",
        ")\n",
        "if env_config.get('num_agents', 1) == 1:\n",
        "    raw_env = SingleAgentWrapper(raw_env)\n",
        "\n",
        "eval_env_test = DummyVecEnv([lambda: raw_env])\n",
        "eval_env_test = VecTransposeImage(eval_env_test)\n",
        "\n",
        "for episode in range(n_eval_episodes):\n",
        "    obs = eval_env_test.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    episode_length = 0\n",
        "    \n",
        "    while not done:\n",
        "        action, _ = trained_model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, info = eval_env_test.step(action)\n",
        "        episode_reward += float(reward[0])\n",
        "        episode_length += 1\n",
        "    \n",
        "    episode_rewards.append(episode_reward)\n",
        "    episode_lengths.append(episode_length)\n",
        "    print(f\"Episode {episode + 1}: Reward={episode_reward:.2f}, Length={episode_length}\")\n",
        "\n",
        "eval_env_test.close()\n",
        "raw_env.close()\n",
        "\n",
        "print(f\"\\nMean Reward: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
        "print(f\"Mean Episode Length: {np.mean(episode_lengths):.1f} ± {np.std(episode_lengths):.1f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Training Progress\n",
        "\n",
        "Load TensorBoard logs to visualize training progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Start TensorBoard\n",
        "%tensorboard --logdir {log_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Results\n",
        "\n",
        "Download trained models and results to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create zip file of results\n",
        "import shutil\n",
        "\n",
        "results_zip = 'racecar_training_results.zip'\n",
        "shutil.make_archive('racecar_training_results', 'zip', model_dir.parent)\n",
        "\n",
        "print(f\"Results zipped: {results_zip}\")\n",
        "print(\"\\nTo download:\")\n",
        "print(\"1. Use the file browser on the left\")\n",
        "print(\"2. Right-click on racecar_training_results.zip\")\n",
        "print(\"3. Select 'Download'\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}